\documentclass{article}

\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate}
\usepackage[linesnumbered, ruled, noend ]{algorithm2e}
\usepackage{hyperref}       % hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    citecolor=blue,
    urlcolor=blue, %cyan
}

\frenchspacing

\title{Scaling Deep Learning with Actors and Asynchronous Tasks}
\author{Eric Liang, Richard Liaw, Zongheng Yang}
\date{May 2018}

\begin{document}

\maketitle
% \section {TODO}

% {\color{red}

% (1) Practical content/creativity; implementation/tuning effort

% (2) Experimental data: scaling/performance analysis/interesting inputs or outputs

% (3) Theoretic content/creativity; design/analysis of algorithms

% (4) Impact: difficulty and timeliness of the contribution

% We understand that not all projects will have all four components but it is expected that the first two components will be present in all projects. As such, we expect the majority of your final report to be about parallelism, and not the problem description. }

\section{Introduction}
This project investigates supporting distributed deep learning training, using \textit{actors} and \textit{asynchronous tasks}, two programming models that are not commonly employed in these workloads.  In contrast, many state-of-the-art deep learning systems nowadays adopt the single-program, multiple-data (SPMD) programming model---examples include Distributed TensorFlow~\cite{tensorflow-osdi16} and Horovod~\cite{horovod}.  Qualitatively, decades of conventional wisdom in message-passing frameworks suggests that the SPMD paradigm is harder to program and debug yet can be highly efficient (e.g., MPI~\cite{MPI}).  Through this project, we show that a much more flexible and high-level programming model, based on tasks and actors, is able to match the performance of these specialized systems in neural network distributed training.  Key to our solution is making use of classical techniques seen in HPC and computer systems, such as maximizing the pipelining between computation and communication.

{\bf Problem setup.} For the remainder of the paper, we constrain the problem setting to neural network (NN) training using multiple machines each equipped with multiple GPUs.  All experiments consider the image classification model, \texttt{ResNet101}~\cite{resnet}, on synthetic dataset with the same format as ImageNet~\cite{imagenet}.  The key performance metric is \textit{throughput}, or total images processed (i.e., a forward and a corresponding backward pass have been performed) per second.

\subsection{Parallelism in Distributed Deep Learning}
Distributed deep learning was popularized with the introduction of parameter servers ~\cite{param-server}, which are centralized and have state that is updated asynchronously among workers. With this lack of coordination comes a price - the inability to scale infinitely due to process noise introduced by the asynchrony.

Recent work has moved towards using synchronous updates for SGD as seen in Horovod~\cite{horovod}, utilizing a variety of high performance computing techniques (ie, allreduce) along with novel parameter tuning to come up with new implementations of SGD to use both more machines and achieve faster learning.

For a more complete treatment of different forms of parallelisms in deep learning, we refer the reader to the survey by Ben-Nun et al.~\cite{ben-nun18_demys_paral_distr_deep_learn}.

\subsection{Parallelism via Actors and Tasks}
We implement all experiments in Ray~\cite{ray}, a new distributed execution engine that supports \textit{actors} and \textit{tasks}.  This section briefly describes the two constructs and how they specifically relate to distributed SGD.
Table~\ref{table:tasks-vs-actors} summarizes the properties of tasks and actors.

\begin{table}[tbh]
\begin{center}
\begin{footnotesize}
\begin{tabular}{| c | c |}\hline
{\bf Tasks (stateless)} & {\bf Actors (stateful)} \\\hline
    Fine-grained load balancing & Coarse-grained load balancing \\\hline
    Support for object locality & Poor locality support \\\hline
    High overhead for small updates & Low overhead for small updates \\\hline
    Efficient failure handling & Overhead from checkpointing \\\hline
    % Application & light-weight simulations, & training, 3rd party \\
    %  examples   & input pre-processing  & stateful simulators \\\hline
\end{tabular}
\end{footnotesize}
\end{center}
\vspace{-0.6cm}
    \caption{\small{Tasks vs. actors tradeoffs.}}
\label{table:tasks-vs-actors}
\end{table}

{\bf Actors.} An \emph{actor} is a stateful process. Each actor exposes methods that
%is a stateful process that exposes a set
can be invoked remotely and are executed serially.
A method execution is similar to a task, in that it executes remotely and returns a future, but differs in that it executes on
a {\em stateful} worker. A {\em handle} to an actor can be passed to other actors or
tasks, making it possible for them to invoke methods on that actor.

\textbf{Actors and SGD}
The most important role of an actor is to hold state that is expensive to initialize. For SGD, this is the TensorFlow session and GPU context, which take many seconds to create. Since the forward and backward pass for the ResNet101 model can be computed in a few hundred milliseconds, it is impractical for SGD to be implemented in pure task-based systems, e.g. Spark or MapReduce.

For each node, we create a single SGD actor that holds a multi-GPU TensorFlow graph and associated GPU state. Depending on the distributed SGD strategy, additional actors and tasks are used to synchronize gradients between SGD actors. It is important to note that, unlike in MPI and other SPMD frameworks, each actor does not manage its own execution. Rather, \textit{tasks are scheduled onto actors} by a higher-level actor (or a \textit{driver} program) callin actor methods.

{\bf Tasks.} Tasks more naturally model stateless computations. While it is possible to use tasks in conjunction with actors, in this project we stick with using just actors.

% Remote functions operate on immutable objects and are expected to be
% \emph{stateless} and side-effect free: their outputs are determined solely by
% their inputs. This implies idempotence, which simplifies fault tolerance through function re-execution on failure.

% Tasks enable fine-grained load balancing through
% leveraging load-aware scheduling at task granularity, input data locality, as each task can be scheduled on the node storing its inputs, and low recovery overhead, as there is no need to checkpoint and recover intermediate state. In contrast, actors provide much more efficient fine-grained updates, as these updates are performed on internal rather than external state, which typically requires serialization and deserialization. For example, actors can be used to implement parameter servers~\citep{param-server} and GPU-based iterative computations (e.g., training). In addition, actors can be used to wrap third-party simulators and other opaque handles that are hard to serialize.

\section{Exploring Distributed TensorFlow and Horovod}

Our goal is to match or exceed the performance of current state of the art frameworks. Horovod is a popular distributed SGD implementation which claims near linear scalability to 128 GPUs. They do this by leveraging the ring-allreduce algorithm to synchronize gradients between workers. The algorithm is roughly as follows:

\textbf{Pseudocode.}
\begin{algorithm}
\caption{All-Reduce SGD}
\KwData{Input Data}
\For{all Nodes (simultaneously)} {
\For{all GPUs (simultaneously)}{
	instantiate data\;
	instantiate model replica\;
}
}
Barrier\;

\For{all Nodes (simultaneously)} {
\For{all GPUs (simultaneously)}{
	compute gradient based on data\;
	allreduce(gradient)\;
}
}


\For{all Nodes (simultaneously)} {
\For{all GPUs (simultaneously)}{
	apply gradient to model\;
}
}
\end{algorithm}

Distributed TensorFlow offers a variety of different distributed SGD implementations. For benchmarking we use a synchronized parameter server to reduce across nodes and an averaging scheme within node across GPUs. The algorithm is roughly as follows:

\textbf{Pseudocode.}
\begin{algorithm}
\caption{Synchronous Parameter Server SGD}
\KwData{Input Data, K Nodes}
\For{all Nodes (simultaneously)} {
\For{all GPUs (simultaneously)}{
	instantiate data\;
	instantiate model replica\;
}
}
generate N parameter server shards\;
\For{each shard in the N shards} {
    start parameter server on a different node \;
}
Barrier\;
\For{all Nodes (simultaneously)} {
\For{all GPUs (simultaneously)}{
	compute gradient based on data\;
	allreduce(gradient) on node\;
}
send shard of gradient to corresponding parameter server \;
wait for aggregated gradient \;
apply gradient to model \;
}
\For{all Parameter servers (simultaneously)}{
    \While {number of gradient shards $<$ K\;}{
        wait for gradient shard\;
    }

    aggregate all gradients\;
    broadcast gradients back to workers\;
}
\end{algorithm}

Note that in both cases, the amount of data transferred is constant \textit{per node} and is equal to $2 * N$, where $N$ is the size of the gradient. The total amount of bandwidth consumed scales linearly with the number of nodes.

To understand this, consider ring reduce. For $k$ participants, the gradient buffer is split into $k$ pieces, and each nodes sends and receives $N / k$ bytes over $k$ rounds. In total this means each node sends and receives $2 * N$ bytes.

A similar accounting holds for the parameter server. Each gradient produce must send $N$ bytes to the parameter server, and receive $N$ bytes from the parameter server. If the gradient is sharded into $k$ shards, then the network usage can be balanced evenly across all machines.

The main tradeoff between the ring reduce and shareded parameter server approach then, is the communication pattern itself. The parameter server requires all-to-all network communication, while ring reduce only requires neighboring nodes to communicate. However at the scales we are dealing with (up to a dozen nodes and a hundred GPUs), this is unlikely to be a bottleneck.

\include{impl}
\include{eval}


\section{Future Work}
{\color{red} TODO}


{
\footnotesize
\bibliographystyle{acm}
\bibliography{ray}
}

\end{document}
