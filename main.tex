\documentclass{article}

\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate}
\usepackage[linesnumbered, ruled, noend ]{algorithm2e}
\SetAlFnt{\small}
% \SetAlCapFnt{\large}
% \SetAlCapNameFnt{\large}
\usepackage{hyperref}       % hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    citecolor=blue,
    urlcolor=blue, %cyan
}
% \usepackage{subfig}
\usepackage{subcaption}

\frenchspacing

\title{Scaling Deep Learning with Actors and Asynchronous Tasks}
\author{Eric Liang, Richard Liaw, Zongheng Yang}
\date{May 2018}

\begin{document}

\maketitle
% \section {TODO}

% {\color{red}

% (1) Practical content/creativity; implementation/tuning effort

% (2) Experimental data: scaling/performance analysis/interesting inputs or outputs

% (3) Theoretic content/creativity; design/analysis of algorithms

% (4) Impact: difficulty and timeliness of the contribution

% We understand that not all projects will have all four components but it is expected that the first two components will be present in all projects. As such, we expect the majority of your final report to be about parallelism, and not the problem description. }

\section{Introduction}
This project investigates supporting distributed deep learning training, using \textit{actors} and \textit{asynchronous tasks}, two programming models that are not commonly employed in these workloads.  In contrast, many state-of-the-art deep learning systems nowadays adopt the single-program, multiple-data (SPMD) programming model---examples include Distributed TensorFlow~\cite{tensorflow-osdi16} and Horovod~\cite{horovod}.  Qualitatively, decades of conventional wisdom in message-passing frameworks suggests that the SPMD paradigm is harder to program and debug yet can be highly efficient (e.g., MPI~\cite{MPI}).  Through this project, we show that a much more flexible and high-level programming model, based on tasks and actors, is able to match the performance of these specialized systems in neural network distributed training.  Key to our solution is making use of classical techniques seen in HPC and computer systems, such as maximizing the pipelining between computation and communication.

{\bf Problem setup.} For the remainder of the paper, we constrain the problem setting to neural network (NN) training using multiple machines each equipped with multiple GPUs.  All experiments consider the image classification model, \texttt{ResNet101}~\cite{resnet}, on synthetic dataset with the same format as ImageNet~\cite{imagenet}.  The key performance metric is \textit{throughput}, or total images processed (i.e., a forward and a corresponding backward pass have been performed) per second.

\subsection{Parallelism in Distributed Deep Learning}
Distributed deep learning was popularized with the introduction of parameter servers~\cite{param-server}, which are centralized and have state that is updated asynchronously among workers. With this lack of coordination comes a price - the inability to scale infinitely due to process noise introduced by the asynchrony.

Recent work has moved towards using synchronous updates for SGD as seen in Horovod~\cite{horovod}, utilizing a variety of high performance computing techniques (i.e., allreduce) along with novel parameter tuning to come up with new implementations of SGD to use both more machines and achieve faster learning.

For a more complete treatment of different forms of parallelisms in deep learning, we refer the reader to the survey by Ben-Nun et al.~\cite{ben-nun18_demys_paral_distr_deep_learn}.

\subsection{Parallelism via Actors and Tasks}
We implement all experiments in Ray~\cite{ray}, a new distributed execution engine that supports \textit{actors} and \textit{tasks}.  This section briefly describes the two constructs and how they specifically relate to distributed SGD.
%Table~\ref{table:tasks-vs-actors} summarizes the properties of tasks and actors.

%\begin{table}[tbh]
%\begin{center}
%\begin{footnotesize}
%\begin{tabular}{| c | c |}\hline
%{\bf Tasks (stateless)} & {\bf Actors (stateful)} \\\hline
%    Fine-grained load balancing & Coarse-grained load balancing \\\hline
%    Support for object locality & Poor locality support \\\hline
%    High overhead for small updates & Low overhead for small updates \\\hline
%    Efficient failure handling & Overhead from checkpointing \\\hline
%    % Application & light-weight simulations, & training, 3rd party \\
%    %  examples   & input pre-processing  & stateful simulators \\\hline
%\end{tabular}
%\end{footnotesize}
%\end{center}
%\vspace{-0.6cm}
%    \caption{\small{Tasks vs. actors tradeoffs.}}
%\label{table:tasks-vs-actors}
%\end{table}

{\bf Ray Actors.} An \emph{actor} is a stateful process. Each actor exposes methods that
%is a stateful process that exposes a set
can be invoked remotely and are executed serially.
A method execution is similar to a task, in that it executes remotely and returns a future, but differs in that it executes on
a {\em stateful} worker. A {\em handle} to an actor can be passed to other actors or
tasks, making it possible for them to invoke methods on that actor.

\textbf{Actors and SGD}
The most important role of an actor is to hold state that is expensive to initialize. For SGD, this is the TensorFlow session and GPU context, which take many seconds to create. Since the forward and backward pass for the ResNet101 model can be computed in a few hundred milliseconds, it is impractical for SGD to be implemented in pure task-based systems, e.g. Spark or MapReduce.

For each node, we create a single SGD actor that holds a multi-GPU TensorFlow graph and associated GPU state. Depending on the distributed SGD strategy, additional actors and tasks are used to synchronize gradients between SGD actors. It is important to note that, unlike in MPI and other SPMD frameworks, each actor does not manage its own execution. Rather, \textit{tasks are scheduled onto actors} by a higher-level actor (or a \textit{driver} program) calling actor methods.

{\bf Tasks.} Tasks more naturally model stateless computations. While it is possible to use tasks in conjunction with actors, in this project we stick with using just actors.

% Remote functions operate on immutable objects and are expected to be
% \emph{stateless} and side-effect free: their outputs are determined solely by
% their inputs. This implies idempotence, which simplifies fault tolerance through function re-execution on failure.

% Tasks enable fine-grained load balancing through
% leveraging load-aware scheduling at task granularity, input data locality, as each task can be scheduled on the node storing its inputs, and low recovery overhead, as there is no need to checkpoint and recover intermediate state. In contrast, actors provide much more efficient fine-grained updates, as these updates are performed on internal rather than external state, which typically requires serialization and deserialization. For example, actors can be used to implement parameter servers~\citep{param-server} and GPU-based iterative computations (e.g., training). In addition, actors can be used to wrap third-party simulators and other opaque handles that are hard to serialize.
\begin{figure}
    \centering
    % \includegraphics[width=3.1in,keepaspectratio]{fig/sgd_plot.pdf}
    \includegraphics[width=3.1in,keepaspectratio]{fig/pseudocode.png}
    \caption{
    \small{
        Pseudocode of SGD implemented with Ray actors. In contrast with MPI-like
        programming models, all control flow is managed by the top-level program.
    }
    }
    \label{fig:code}
\end{figure}

\input{exploring}
\input{impl}
\input{eval}

\newpage
{
\footnotesize
\bibliographystyle{acm}
\bibliography{ray}
}

\end{document}
